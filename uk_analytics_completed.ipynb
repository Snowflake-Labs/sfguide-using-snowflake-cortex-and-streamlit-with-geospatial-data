{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lastEditStatus": {
   "notebookId": "4ghzjee65frksxaciojj",
   "authorId": "3234949799362",
   "authorName": "BECKY2",
   "authorEmail": "becky.oconnor@snowflake.com",
   "sessionId": "db2f0485-4e6b-4192-8f55-6643baa3c959",
   "lastEditTime": 1737157368912
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c28cd4c-d498-4888-9a1c-0cafa33a1f34",
   "metadata": {
    "name": "intro",
    "collapsed": false,
    "resultHeight": 743
   },
   "source": "# Impact of Events to local Transport links\nAnalyzing Location specific data in order to make decisions often requires ‘niche' techniques which are often worked on by ‘geography' experts. In snowflake, everyone can be an expert in any location in which they need to understand - whether it's points of interest, transport links or government boundaries - all is feasible using standard functionality available within Snowflake.\n\nAlready, there are so many location specific datasets available within the Snowflake Marketplace which significantly reduces data ingestion and engineering time. Getting access to these datasets are as simple as Get Data, where you will enjoy leveraging location specific features using either SQL Queries or Snowpark Dataframes. And these result sets are then rendered easily using Streamlit in Snowflake \ndashboards.\n\n**Snowflake Cortex LLMs** - are used to save the analyst, engineer or even BI developer time - which can help a multitude of tasks - from improving the readability of location tooltips, to the generation or synthetic data for testing purposes.\n\nIn this quickstart, we will be leveraging the the tools within Snowflake to:\n\n-   **Visualise** the location of train stations within the north of england and understand where nearby restaurants are located\n-   **Discover** where the locations of Large events are and where they may impact stations and Restaurants\n-   **Understand** the weather conditions which might impact train stations\n-   **Generate** warning letter to the MP after discovering potential risk synthezised events which might happen and will impact services\nVisualise the data using Streamlit\n\n## What You'll Learn\n- An understanding of Geospatial data in Snowflake\n- Using Cortex functions with Snowpark\n- Creating a location centric application using Streamlit\n- An insight to location centric Datasets\n- Using Notebooks and Streamlit to make discoveries and present findings"
  },
  {
   "cell_type": "markdown",
   "id": "7e912cb7-0296-4e54-840a-db1e6a2f8220",
   "metadata": {
    "name": "instructions_prereq",
    "resultHeight": 307,
    "collapsed": false
   },
   "source": "## Before Running this notebook. \nAdd the following datasets from the marketplace\n\n- Carto Overture Maps Places Dataset\n- Northern Trains station dataset\n\n## Next \n- Ensure the public role is added to the datasets\n- run throuth the notebook step by step."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8264ba-dca5-4571-ac2f-b595f8c4b5a3",
   "metadata": {
    "name": "cell2",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "GRANT IMPORTED PRIVILEGES ON DATABASE NORTHERN_TRAINS_STATION_DATA TO ROLE PUBLIC;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE OVERTURE_MAPS__PLACES TO ROLE PUBLIC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63345733-322f-431b-9556-c8a91018aaab",
   "metadata": {
    "name": "narritive_0",
    "collapsed": false,
    "resultHeight": 153
   },
   "source": "## Creating your first map layer\nBefore we start creating maps - we will import all the libraries we need.  This notebook comes pre-installed with **pydeck** which we will utilise for the majority our map creations. There is actually a very simple quick map if you are simply needing to plot points.  This we will use **st.map** which is part of the streamlit suite of products."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24250f-24e5-48e4-8a6e-e43a529cb8d0",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "libraries",
    "resultHeight": 115
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import *\n",
    "from snowflake.snowpark.types import *\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydeck as pdk\n",
    "\n",
    "# Write directly to the app\n",
    "st.title(\"UK Analytics within the North of England :train:\")\n",
    "st.write(\n",
    "    \"\"\"This app shows key insight of places and events that may effect Northern Trains.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Get the current credentials\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54510837-3f22-4913-bc56-d50faca2c1da",
   "metadata": {
    "name": "narrative_1",
    "collapsed": false,
    "resultHeight": 118
   },
   "source": "We will firstly leverage the Northern Trains dataset to filter the Carto Overture maps places dataset. We want to do this in order to get Points of interests that are relevant for the Northern Trains locality. Joining by each Station Code would be resource hungry - plus we do not want to join by exact locations, only by roughly where all of the train stations are situated.  So running the nect cell will not only create a data frame that contains all the train stations, it will also plot where they are on a map using - as previously mentioned **st.map**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae35d1e-9df6-486f-bdb7-e20e2996319b",
   "metadata": {
    "language": "python",
    "name": "n_trains_Data",
    "resultHeight": 993,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trains_latlon = session.table('NORTHERN_TRAINS_STATION_DATA.TESTING.\"StationLatLong\"')\n",
    "\n",
    "st.markdown('#### A dataframe which shows all the train stations')\n",
    "st.dataframe(trains_latlon)\n",
    "st.map(trains_latlon, latitude='Latitude', longitude='Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949306d-5e2c-4b9d-b088-2f73d4674d80",
   "metadata": {
    "name": "narrative2",
    "collapsed": false,
    "resultHeight": 139
   },
   "source": "### Create a Boundary Box to filtering purposes\n\nYou previously loaded the places dataset from Carto Overture maps. This dataset offers a comprehensive list of places of interest across the world such as restaurants, bars and schools. We want to filter this dataset to only list places of interest that occur within the Northern Trains locality. Creating a Boundary box is the easiest option."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50193e2a-6458-444c-8678-6bfa2843ed85",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "boundary_box",
    "resultHeight": 636
   },
   "outputs": [],
   "source": [
    "#create a point from the coordinates\n",
    "envelope = trains_latlon.with_column('POINT',call_function('ST_MAKEPOINT',col('\"Longitude\"'),col('\"Latitude\"')))\n",
    "\n",
    "#collect all the points into one row of data\n",
    "envelope = envelope.select(call_function('ST_COLLECT',col('POINT')).alias('POINTS'))\n",
    "\n",
    "#### convert from geography to geometry\n",
    "envelope = envelope.select(to_geometry('POINTS').alias('POINTS'))\n",
    "\n",
    "\n",
    "#create a rectangular shape which boarders the minimum possible size which covers all of the points\n",
    "envelope = envelope.select(call_function('ST_ENVELOPE',col('POINTS')).alias('BOUNDARY'))\n",
    "\n",
    "#convert back to geography\n",
    "envelope = envelope.select(to_geography('BOUNDARY').alias('BOUNDARY'))\n",
    "envelope.collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fef73-7eb7-4941-8de4-58c8ddb3bff1",
   "metadata": {
    "name": "narrative_3",
    "collapsed": false,
    "resultHeight": 176
   },
   "source": "You will see this has generated 5 sets of coordinates in order to draw the boundary box.\n\n**FACT**: Every valid polygon will have the same last pair of coordinates as the first pair. Lets visualise what this looks like using the library pydeck. Although st.map is useful for simple quick visualisation of points, pydeck has the ability to visualise lines, points and polygons in 2D and 3D. It also has layer options for lines, points, icons and H3 indexes.\n\nhttps://deckgl.readthedocs.io/en/latest/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a41783-d7c6-4ab6-8a53-557b6ecdab76",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "boundary_layer",
    "resultHeight": 423
   },
   "outputs": [],
   "source": [
    "#find the centre point so the map will render from that location\n",
    "\n",
    "centre = envelope.with_column('CENTROID',call_function('ST_CENTROID',col('BOUNDARY')))\n",
    "centre = centre.with_column('LON',call_function('ST_X',col('CENTROID')))\n",
    "centre = centre.with_column('LAT',call_function('ST_Y',col('CENTROID')))\n",
    "\n",
    "#create LON and LAT variables\n",
    "\n",
    "centrepd = centre.select('LON','LAT').to_pandas()\n",
    "LON = centrepd.LON.iloc[0]\n",
    "LAT = centrepd.LAT.iloc[0]\n",
    "\n",
    "### transform the data in pandas so the pydeck visualisation tool can view it as a polygon\n",
    "\n",
    "envelopepd = envelope.to_pandas()\n",
    "envelopepd[\"coordinates\"] = envelopepd[\"BOUNDARY\"].apply(lambda row: json.loads(row)[\"coordinates\"][0])\n",
    "\n",
    "\n",
    "####visualise on a map\n",
    "\n",
    "#### create a layer - this layer will visualise the rectangle\n",
    "\n",
    "polygon_layer = pdk.Layer(\n",
    "            \"PolygonLayer\",\n",
    "            envelopepd,\n",
    "            opacity=0.3,\n",
    "            get_polygon=\"coordinates\",\n",
    "            filled=True,\n",
    "            get_fill_color=[16, 14, 40],\n",
    "            auto_highlight=True,\n",
    "            pickable=False,\n",
    "        )\n",
    "\n",
    " \n",
    "#### render the map \n",
    "    \n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=400\n",
    "        ),\n",
    "    \n",
    "layers= [polygon_layer]\n",
    "\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8010a-5fad-4b06-a552-4f77e5c45b15",
   "metadata": {
    "name": "narritive_4",
    "collapsed": false,
    "resultHeight": 273
   },
   "source": "You will see that to render the map, we present the data in a format for pydeck to accurately read. The final transformed dataset is a pandas dDataframe. We specify the dataframe in a pydeck layer, then apply this layer to a streamlit pydeck chart. If we want, we can use the same logic in order to create a streamlit app. Snowflake Notebooks are great as you can render streamlit on the fly - without having to run the ‘app' separately.\n\n### Filtering the data using the boundary box\n\nNext, lets leverage and filter the overture maps so these will only consist of data within this area. Overture maps consist of location data across the entire globe.\n\nWhen you run the cell, you will see there is a lot of semi structured data returned. We will use snowflake's native semi structured querying capability to take key elements out of the data which includes information concerning the location"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15848ee6-006a-4f8b-ba5a-d1f1487061ae",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "places_dataset",
    "resultHeight": 181
   },
   "outputs": [],
   "source": [
    "places_1 = session.table('OVERTURE_MAPS__PLACES.CARTO.PLACE')\n",
    "places_1 = places_1.filter(col('ADDRESSES')['list'][0]['element']['country'] =='GB')\n",
    "\n",
    "places_1.limit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f1d49-c5b8-411b-a2c9-3f049b6d9d0b",
   "metadata": {
    "name": "narritive_5",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Here is the dataset in a more readable format"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1036ff9-00aa-4bfc-8fce-22fc54685cd5",
   "metadata": {
    "language": "python",
    "name": "places_refined",
    "resultHeight": 426,
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "places_2 = places_1.select(col('NAMES')['primary'].astype(StringType()).alias('NAME'),\n",
    "                        col('PHONES')['list'][0]['element'].astype(StringType()).alias('PHONE'),\n",
    "                      col('CATEGORIES')['primary'].astype(StringType()).alias('CATEGORY'),\n",
    "                        col('CATEGORIES')['alternate']['list'][0]['element'].astype(StringType()).alias('ALTERNATE'),\n",
    "                    col('websites')['list'][0]['element'].astype(StringType()).alias('WEBSITE'),\n",
    "                      col('GEOMETRY'))\n",
    "                        \n",
    "\n",
    "places_2.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad7f04-b3e8-4e2f-b297-c7be33519688",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "places_filtered_boundary",
    "resultHeight": 410,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "places_3 = places_2.filter(col('CATEGORY') =='restaurant') ##changed fron train_station to restaurant\n\nplaces_3 = places_3.join(envelope,call_function('ST_WITHIN',places_3['GEOMETRY'],envelope['boundary']))\nplaces_3 = places_3.with_column('LON',call_function('ST_X',col('GEOMETRY')))\nplaces_3 = places_3.with_column('LAT',call_function('ST_Y',col('GEOMETRY')))\nst.write(places_3.limit(10))"
  },
  {
   "cell_type": "markdown",
   "id": "49897ec0-3800-4d42-9055-081e0d1db9a4",
   "metadata": {
    "name": "narritive_6",
    "collapsed": false,
    "resultHeight": 139
   },
   "source": "### Viewing Multiple Layers\nWe can view the points on a map easily by using st.map(places) but as pydeck has many more options such as different mark types, tool tips and layers we will create an additional pydeck layer which adds this data to the previously created data layer. When you hover over in the boundary box you will see a tooltip containing the alternate category as well as the place name."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab963c-2c8a-4b14-a425-79ae2e0ff86c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "places_visualised",
    "resultHeight": 423,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "placespd = places_3.to_pandas()\n",
    "poi_l = pdk.Layer(\n",
    "            'ScatterplotLayer',\n",
    "            data=placespd,\n",
    "            get_position='[LON, LAT]',\n",
    "            get_color='[255,255,255]',\n",
    "            get_radius=600,\n",
    "            pickable=True)\n",
    "\n",
    "#### render the map showing trainstations based on overture maps\n",
    "    \n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=400\n",
    "        ),\n",
    "    \n",
    "layers= [polygon_layer, poi_l], tooltip = {'text':\"Place Name: {NAME}, alternate: {ALTERNATE}\"}\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad164163-f3c9-436d-a0b3-13b7c1651894",
   "metadata": {
    "name": "narrative_7",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Now we have a map with all the restaurants within the Northern trains boundary. Lets now compare this with another layer which shows the train stations provided by Northern Trains. We have already loaded the train station locations into the notebook when we created the boundary box"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb5c5b-9190-4143-8667-20f7bb1234e8",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "trains_visualised",
    "resultHeight": 423
   },
   "outputs": [],
   "source": [
    "\n",
    "trains_latlon_renamed = trains_latlon\n",
    "\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"CrsCode\"','NAME')\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"Postcode\"','ALTERNATE')\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"Latitude\"','LAT')\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"Longitude\"','LON')\n",
    "trains_latlon_renamed_pd = trains_latlon_renamed.to_pandas()\n",
    "\n",
    "nw_trains_l = pdk.Layer(\n",
    "            'ScatterplotLayer',\n",
    "            data=trains_latlon_renamed_pd,\n",
    "            get_position='[LON, LAT]',\n",
    "            get_color='[0,187,255]',\n",
    "            get_radius=600,\n",
    "            pickable=True)\n",
    "\n",
    "#### render the map showing trainstations based on overture maps\n",
    "    \n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=400\n",
    "        ),\n",
    "    \n",
    "layers= [polygon_layer, poi_l, nw_trains_l], tooltip = {'text':\"Place Name: {NAME}, alternate: {ALTERNATE}\"}\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f87093-fd44-4501-9cb4-5d8024a72069",
   "metadata": {
    "name": "narrative_8",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "We have now rendered a multi layer map which overlays restaurants and northern rail train stations. Next, we will leverage Cortex to curate descriptive tooltips derived by station attributes.  \n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b34c7d-5d81-41e1-829a-85394d16bd56",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "station_attributes",
    "resultHeight": 216
   },
   "outputs": [],
   "source": [
    "further_train_info_1 = session.table('NORTHERN_TRAINS_STATION_DATA.TESTING.\"STATION ATTRIBUTES 2\"')\n",
    "further_train_info_1.limit(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e8e43-0d43-4a56-81f8-923e0ad73f38",
   "metadata": {
    "name": "narrative_9",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "We have quite a bit of information, it would be great if we can use Snowflake Cortex LLM to explain this data and then we could add the results to our tool tip!! Right now we only have the postcode in the tooltip."
  },
  {
   "cell_type": "markdown",
   "id": "b2896a6a-df7e-4c28-bb80-915133e45f17",
   "metadata": {
    "collapsed": false,
    "name": "cortex_description",
    "resultHeight": 67
   },
   "source": "Below we are leveraging Mistral-large2 to produce meaningful tooltips relating to over **400** train stations which are managed by **Northern Trains**.  This takes around 1.5 minutes to complete as it will write a summary for all 400 train stations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f969e-fa87-4c80-9632-c0ff0852b966",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cortex_tooltip",
    "resultHeight": 251
   },
   "outputs": [],
   "source": [
    "\n",
    "further_train_info_2= further_train_info_1.with_column('OBJECT',object_construct(lit('CRS Code'),\n",
    "col('\"CRS Code\"'),\n",
    "lit('Full Timetable Calls'),\n",
    "col('\"Dec21 Weekday Full Timetable Daily Calls\"').astype(IntegerType()),\n",
    "lit('Emergency Timetable Calls'),\n",
    "col('\"Dec21 Weekday Emergency Timetable Daily Calls\"').astype(IntegerType()),\n",
    "lit('Footfall'),\n",
    "col( '\"ORR Station Footfall 2020-21\"').astype(IntegerType()),\n",
    "lit('Parking'),\n",
    "col('\"Car Parking - Free/Chargeable\"'),\n",
    "lit('MP'),\n",
    "col(\"MP\"),\n",
    "lit(\"Political Party\"),\n",
    "col('\"Political Party\"'),\n",
    "lit('MP Email Address'),\n",
    "col('\"MP Email Address\"'),                                                                             \n",
    "lit('Car Parking Spaces'),\n",
    "col('\"Car Parking Spaces\"').astype(IntegerType()),\n",
    "lit('Staffed?'),\n",
    "col('\"Staffed?\"'))).cache_result()\n",
    "\n",
    "prompt = 'In less than 200 words, write a summary based on the following train station details.  \\\n",
    "The trainstations are based in the North of England. \\\n",
    "Only include Northern train station names in the description.'\n",
    "prompt2 = 'write in the best way for it to describe a point on a map.'\n",
    "\n",
    "further_train_info_2 = further_train_info_2.select('\"CRS Code\"',\n",
    "        'MP',\n",
    "        '\"Political Party\"', \n",
    "        '\"MP Email Address\"',\n",
    "        call_function('snowflake.cortex.complete','mistral-large2',\n",
    "            concat(lit(prompt),\n",
    "            col('OBJECT').astype(StringType()),\n",
    "            lit('prompt2'))).alias('ALTERNATE'))\n",
    "\n",
    "further_train_info_2.write.mode('overwrite').save_as_table(\"DATA.TRAIN_STATION_INFORMATION\")\n",
    "station_info = session.table('DATA.TRAIN_STATION_INFORMATION')\n",
    "station_info.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c9ae2-ca64-4f8e-8d22-139ee889e2c2",
   "metadata": {
    "name": "narrative_10",
    "collapsed": false,
    "resultHeight": 134
   },
   "source": "We used **call_function** to call Snowflake Cortex complete which returns a response that completes an input prompt. Snowflake Cortex runs LLMs that are fully hosted and managed by Snowflake, requiring no setup. In this example, we are using Mistal Large, an open enterprise-grade LLM model managed by Snowflake.\n\nOk, let's now revise the map."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a50839-2809-4fb3-b079-7a0910590a1c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "visualise_tooltip",
    "resultHeight": 723
   },
   "outputs": [],
   "source": [
    "trains_latlon_renamed = trains_latlon\n",
    "\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"CrsCode\"','NAME')\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"Latitude\"','LAT')\n",
    "trains_latlon_renamed = trains_latlon_renamed.with_column_renamed('\"Longitude\"','LON')\n",
    "\n",
    "station_info = session.table('DATA.TRAIN_STATION_INFORMATION')\n",
    "\n",
    "trains_latlon_renamed = trains_latlon_renamed.join(station_info,station_info['\"CRS Code\"']==trains_latlon_renamed['NAME']).drop('\"CRS Code\"')\n",
    "trains_latlon_renamed_pd = trains_latlon_renamed.to_pandas()\n",
    "\n",
    "nw_trains_l = pdk.Layer(\n",
    "            'ScatterplotLayer',\n",
    "            data=trains_latlon_renamed_pd,\n",
    "            get_position='[LON, LAT]',\n",
    "            get_color='[0,187,2]',\n",
    "            get_radius=600,\n",
    "            pickable=True)\n",
    "\n",
    "#### render the map showing trainstations based on overture maps\n",
    "\n",
    "tooltip = {\n",
    "   \"html\": \"\"\"<b>Name:</b> {NAME} <br> <b>Alternate:</b> {ALTERNATE}\"\"\",\n",
    "   \"style\": {\n",
    "       \"width\":\"50%\",\n",
    "        \"backgroundColor\": \"steelblue\",\n",
    "        \"color\": \"white\",\n",
    "       \"text-wrap\": \"balance\"\n",
    "   }\n",
    "}\n",
    "    \n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=700\n",
    "        ),\n",
    "    \n",
    "layers= [polygon_layer, poi_l, nw_trains_l], tooltip = tooltip\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f85983-13e0-4fdf-8c8d-7838590f7846",
   "metadata": {
    "name": "narrative_11",
    "collapsed": false,
    "resultHeight": 139
   },
   "source": "### Use Cortex to list Key location events\n\nAny location may be impacted by key events. Let's try and pinpoint out any key event happening in the north of England and how restaurants and train stations may be impacted by this. We do not have specific event data for this, so in this case, we will leverage Snowflake Cortex and Snowflake Arctic to suggest events that may impact this area. Arctic is not a live data repository - it simply retrieves data back based on trained history within the model.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc17a1-e0d5-4022-83e1-be99e5b6858a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cortex_events",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "json1 = '''{\"DATE\":\"YYYY-MM-DD\", \"NAME\":\"event\",DESCRIPTION:\"describe what the event is\" \"CENTROID\":{\n",
    "  \"coordinates\": [\n",
    "    0.000000<<<this needs to be longitude,\n",
    "    0.000000<<<<this needs to be latitude\n",
    "  ],\n",
    "  \"type\": \"Point\"\n",
    "},\"COLOR\":\"Random bright and unique color in RGB presented in an array\"}'''\n",
    "\n",
    "\n",
    "prompt = f''' Retrieve 6 events within different cities of the north of england and will happen in 2024.  do not include commentary or notes retrive this in the following json format {json1}  '''\n",
    "events_1 = session.create_dataframe([{'prompt':prompt}])\n",
    "\n",
    "events_1 = events_1.select(call_function('SNOWFLAKE.CORTEX.COMPLETE','mistral-large2',prompt).alias('EVENT_DATA'))\n",
    "\n",
    "events_1 = events_1.with_column('EVENT_DATA',replace(col('EVENT_DATA'),'''```json''',''))\n",
    "events_1 = events_1.with_column('EVENT_DATA',replace(col('EVENT_DATA'),'''```''',''))\n",
    "\n",
    "events_1.write.mode('overwrite').save_as_table(\"DATA.EVENTS_IN_THE_NORTH\")\n",
    "session.table('DATA.EVENTS_IN_THE_NORTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb28f0-71ec-4b46-b89e-7e8dcfa27575",
   "metadata": {
    "name": "narrative_12",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Again, we will utilise the semi-structured support in Snowflake to flatten the retrieved json to transpose a data frame in a table format"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d6a0c-1cf5-4b28-b6e7-f59b691bee64",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "events_normalized",
    "resultHeight": 286
   },
   "outputs": [],
   "source": [
    "events_2 = session.table('DATA.EVENTS_IN_THE_NORTH')\n",
    "events_2 = events_2.join_table_function('flatten',parse_json('EVENT_DATA')).select('VALUE')\n",
    "events_2=events_2.with_column('NAME',col('VALUE')['NAME'].astype(StringType()))\n",
    "events_2=events_2.with_column('DESCRIPTION',col('VALUE')['DESCRIPTION'].astype(StringType()))\n",
    "events_2=events_2.with_column('CENTROID',to_geography(col('VALUE')['CENTROID']))\n",
    "events_2=events_2.with_column('COLOR',col('VALUE')['COLOR'])\n",
    "events_2=events_2.with_column('DATE',col('VALUE')['DATE'].astype(DateType())).drop('VALUE')\n",
    "events_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f095c3-393d-45a4-81df-e3ef0a3d13cb",
   "metadata": {
    "name": "narrative_13",
    "collapsed": false,
    "resultHeight": 139
   },
   "source": "### Leveraging H3\n\nWe now have a list of events in a new table. We would like to utilise this to understand the restaurants and train stations which may be impacted. H3 indexes are a way to bucket multiple points into a standardised grid. H3 buckets points into hexagons. Every hexagon at each resolution has a unique index. This index can also be used to join with other datasets which have also been indexed to the same standardised grid. We will do this later in the lab.\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70e8de-559a-48d1-94bb-13a793f7c909",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "h3index",
    "resultHeight": 286
   },
   "outputs": [],
   "source": [
    "\n",
    "events_3=events_2.with_column('H3',call_function('H3_POINT_TO_CELL_STRING',col('CENTROID'),lit(5)))\n",
    "\n",
    "events_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114183e-344c-4576-bc57-bb364bf9eb66",
   "metadata": {
    "name": "narrative_14",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "We will now add these events onto the map as another layer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be21ad-648a-403d-87b1-8ae8c500ac20",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "visualise_h3",
    "resultHeight": 894
   },
   "outputs": [],
   "source": [
    "events = events_3.with_column('R',col('COLOR')[0])\n",
    "events = events.with_column('G',col('COLOR')[1])\n",
    "events = events.with_column('B',col('COLOR')[2])\n",
    "events = events.with_column_renamed('DESCRIPTION','ALTERNATE')\n",
    "eventspd = events.group_by('H3','NAME','ALTERNATE','R','G','B').count().to_pandas()\n",
    "\n",
    "st.write(eventspd)\n",
    "\n",
    "h3_events = pdk.Layer(\n",
    "        \"H3HexagonLayer\",\n",
    "        eventspd,\n",
    "        pickable=True,\n",
    "        stroked=True,\n",
    "        filled=True,\n",
    "        extruded=False,\n",
    "        get_hexagon=\"H3\",\n",
    "        get_fill_color=[\"255-R\",\"255-G\",\"255-B\"],\n",
    "        line_width_min_pixels=2,\n",
    "        opacity=0.4)\n",
    "\n",
    "#### render the map showing trainstations based on overture maps\n",
    "\n",
    "tooltip = {\n",
    "   \"html\": \"\"\"<b>Name:</b> {NAME} <br> <b>Alternate:</b> {ALTERNATE}\"\"\",\n",
    "   \"style\": {\n",
    "       \"width\":\"50%\",\n",
    "        \"backgroundColor\": \"steelblue\",\n",
    "        \"color\": \"white\",\n",
    "       \"text-wrap\": \"balance\"\n",
    "   }\n",
    "}\n",
    "\n",
    "st.pydeck_chart(pdk.Deck(\n",
    "    map_style=None,\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=LAT,\n",
    "        longitude=LON,\n",
    "        zoom=5,\n",
    "        height=600\n",
    "        ),\n",
    "    \n",
    "layers= [polygon_layer, poi_l, h3_events,nw_trains_l, ], tooltip = tooltip\n",
    "\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7b838-da1d-44ae-bcd1-b2abc3c34826",
   "metadata": {
    "name": "narrative_15",
    "collapsed": false,
    "resultHeight": 176
   },
   "source": "So we can see the train stations and restaurants that might be impacted by the events. The events rendered may be different to what is shown in the screenshot. Lets create a dataset that extracts only the impacted areas.\n\nJoin the Events data frame to The Train Stations Data frame. Then, Join the Events Data frame to the Restaurants Data frame.\n\nYou may notice that there are new H3 columns for the restaurants and places of the same resolution as the events. This naturally creates a key to join to. There is also a new column which displays the distance away the restaurant is from the event. This was created using a standard geospatial function."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a030f-563b-4cbb-86fd-cff76ef232f7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "affected_by_events",
    "resultHeight": 600
   },
   "outputs": [],
   "source": [
    "\n",
    "trains_h3 = trains_latlon_renamed.with_column('H3',call_function('H3_LATLNG_TO_CELL_STRING',col('LAT'),col('LON'),lit(5)))\n",
    "trains_h3 = trains_h3.join(events.select('H3',col('NAME').alias('EVENT_NAME'),'DATE'),'H3')\n",
    "\n",
    "st.markdown('#### Affected Train Stations')\n",
    "st.write(trains_h3.limit(1))\n",
    "places_h3 = places_3.with_column('H3',call_function('H3_POINT_TO_CELL_STRING',col('GEOMETRY'),lit(5)))\n",
    "places_h3 = places_h3.join(events.select('H3','CENTROID',col('NAME').alias('EVENT_NAME'),'DATE'),'H3')\n",
    "places_h3 = places_h3.with_column('DISTANCE_FROM_EVENT',call_function('ST_DISTANCE',col('CENTROID'),col('GEOMETRY')))\n",
    "places_h3 = places_h3.filter(col('DISTANCE_FROM_EVENT')< 3000)\n",
    "places_h3 = places_h3.sort(col('DISTANCE_FROM_EVENT').asc())\n",
    "st.markdown('#### Affected Restaurants')                             \n",
    "st.write(places_h3.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48bb4d2-d2e5-4b64-a2d6-68eeca605282",
   "metadata": {
    "name": "narrative_16",
    "collapsed": false,
    "resultHeight": 222
   },
   "source": "We now have all of this joined together - in the next step we will use an LLM to write a letter to each MP which details the concerns which may impact the events.\n\n### Use Cortex to write relevant correspondence\n\nNow that we can see where the events impact stations and restaurants, let's use an LLM to craft a letter to the MP to notify them of these effects. To do this, we need to put all the information needed into objects to easily pass them through the cortex function.\n\nCreate an object which links all affected restaurants to the respective MP. We are also including the distance from the event for each restaurant."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31d0e4-f057-4b5d-8375-96a4b40483fa",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "letter_restaurants",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "object3 = trains_h3.select('H3','MP','\"MP Email Address\"').distinct()\n",
    "object3 = places_h3.join(object3,'H3')  \n",
    "object3 = object3.group_by('MP','\"MP Email Address\"').agg(array_agg(object_construct(lit('NAME'),\n",
    "                                                                col('NAME'),\n",
    "                                                                lit('DISTANCE_FROM_EVENT'),\n",
    "                                                                round('DISTANCE_FROM_EVENT',5).astype(DecimalType(20,4)),\n",
    "                                                                lit('PHONE'),\n",
    "                                                                col('PHONE'),\n",
    "                                                               lit('WEBSITE'),\n",
    "                                                               col('WEBSITE'))).within_group('MP').alias('RESTAURANTS'))\n",
    "object3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18475f-3502-4979-a341-4256b2ba7804",
   "metadata": {
    "name": "narrative_17",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "You will now create another object which links all the affected trains stations to the respective MP. "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513cc8b-c4d0-4d12-8519-e76d1d0b420a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "letter_trains",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "object1 = trains_h3.group_by('MP').agg(array_agg(object_construct(lit('Train Station information'),col('ALTERNATE'))).within_group('MP').alias('TRAIN_STATIONS'))\n",
    "object1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf491a-aa64-4edc-9e89-4c07974f461f",
   "metadata": {
    "name": "narrative_18",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Create an object which links all affected events to the respective MP. Create a new python cell called letter_events and paste the following code into it:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9d9c8-ecd2-4183-93d7-49adc158011d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "letter_events",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "object2 = trains_h3.select('MP','EVENT_NAME','DATE').distinct()\n",
    "object2 = object2.group_by('MP').agg(array_agg(object_construct(lit('EVENT'),col('EVENT_NAME'),lit('DATE'),col('DATE'))).within_group('MP').alias('EVENTS'))\n",
    "object2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31df834-7786-4844-b072-278fdaf62f08",
   "metadata": {
    "name": "narrative_19",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "You will now join all these objects together and persist the results in a table."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91073a-4407-4452-aa2e-64a4afce9be9",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "letter_object",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "all_3 = object1.join(object2,'MP')\n",
    "all_3 = all_3.join(object3,'MP')\n",
    "\n",
    "all_3.write.mode('overwrite').save_as_table(\"DATA.EVENTS_AND_WHAT_IS_AFFECTED\")\n",
    "session.table('DATA.EVENTS_AND_WHAT_IS_AFFECTED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc6389-878d-4954-8809-9c98f6976a66",
   "metadata": {
    "name": "narrative_20",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "The results can include a large number of restaurants by MP, so let's only refer to the first 8 restaurants for each MP letter based on distance from the event. The array_slice method does just that. Create a new Python cell called filt_restaurant_obj and paste the following python content into it:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69e317-cd46-4840-87d6-c1b2cd4b21f8",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "filt_restaurant_obj",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "all_4 = session.table(\"DATA.EVENTS_AND_WHAT_IS_AFFECTED\")\n",
    "all_4 = all_4.select('MP','\"MP Email Address\"','TRAIN_STATIONS','EVENTS',\n",
    "                     \n",
    "array_slice(col('RESTAURANTS'),lit(0),lit(8)).alias('RESTAURANTS'))\n",
    "\n",
    "          \n",
    "all_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925f91c-3159-4b6b-ae34-286cc76a10e9",
   "metadata": {
    "name": "narrative_21",
    "collapsed": false,
    "resultHeight": 113
   },
   "source": "### Generate a suitable Prompt for Cortex LLM powered letter writing\nCreate a prompt for the LLM which pulls all this information together. You may want to change who the letter is written to. The prompt will encourage the letter to be written by Becky.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9bd82-416d-486e-8aaa-5ba8ba8b134b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "prompt_letter",
    "resultHeight": 156
   },
   "outputs": [],
   "source": [
    "col1,col2,col3, col4 = st.columns(4)\n",
    "\n",
    "with col1:\n",
    "    name = st.text_input('Name:','''Becky O'Connor''')\n",
    "with col2:\n",
    "    email = st.text_input('Email:','becky.oconnor@snowflake.com')\n",
    "with col3:\n",
    "    title = st.text_input('Title:','a concerned Citizen')\n",
    "with col4:\n",
    "    style = st.text_input('Style:','a worried resident')\n",
    "\n",
    "\n",
    "prompt = concat(lit('Write an email addressed to this MP:'),\n",
    "                lit('in the style of '),\n",
    "                lit(style),\n",
    "                col('MP'),\n",
    "               lit('about these events: '),\n",
    "               col('EVENTS').astype(StringType()),\n",
    "               lit('effecting these stations: '),\n",
    "               col('TRAIN_STATIONS').astype(StringType()),\n",
    "                lit('And these Restaurants: '),\n",
    "                col('RESTAURANTS').astype(StringType()),\n",
    "               lit(f'''The letter is written by {name} - {email} - {title}'''))\n",
    "\n",
    "st.info(f'''Letters will be generated from {name} - {email} - {title} in the style of {style}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57b3ec-6cec-441e-90e0-59067896a29a",
   "metadata": {
    "name": "narrative_22",
    "collapsed": false,
    "resultHeight": 108
   },
   "source": "Once you run the cell, change the prompts to reflect a sender of your choice.\n\nCall the LLM with the prompt by copying the code below into a new cell. You may want to be creative and change who the letter is written by, or even ask Cortex to write it in the style of someone. The LLM we are using for this is Mixtral-8x7b as its good at writing letters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1834e6d-ab1d-42e8-99ea-f7d97e82bafb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cortex_letter",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "letters = all_4.select('MP','\"MP Email Address\"', call_function('SNOWFLAKE.CORTEX.COMPLETE','mixtral-8x7b',prompt).alias('LETTER'))\n",
    "letters.write.mode('overwrite').save_as_table(\"DATA.LETTERS_TO_MP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365619f-7216-4f42-8b7f-e533f9b9332d",
   "metadata": {
    "name": "narrative_23",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "We have now saved our letters. The next task is to view the letters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffc15f-5a4a-4ec7-9518-696c167de986",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "new_letters",
    "resultHeight": 438
   },
   "outputs": [],
   "source": [
    "letters = session.table('DATA.LETTERS_TO_MP')\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c9bb2-81fa-4ddf-a0cb-acc7a7c2c285",
   "metadata": {
    "name": "narrative_24",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "The code below allows the user to browse the letters with a slider and visualise a letter.Try it out"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a7c5b-5f8b-4033-8138-f1887aef1476",
   "metadata": {
    "language": "python",
    "name": "letter_reader",
    "resultHeight": 827,
    "collapsed": false
   },
   "outputs": [],
   "source": "letterspd = letters.to_pandas()\nselected_letter = st.slider('Choose Letter:',0,letterspd.shape[0]-1,1)\nst.markdown(f''' **Email**: {letterspd['MP Email Address'].iloc[selected_letter]}''')\nst.write()\nst.write(letterspd.LETTER.iloc[selected_letter])\nst.snow()"
  },
  {
   "cell_type": "markdown",
   "id": "e8530c03-8c18-4097-b551-7793de131094",
   "metadata": {
    "name": "narrative_25",
    "collapsed": false,
    "resultHeight": 367
   },
   "source": "So now we have learnt about the restaurants and train stations that might be impacted by these key events and written letters to all the MPs expressing our concerns.  We have also learnt about the following features:\n- Geospatial Functions in Snowflake\n- Semi Structured Data Support\n- Cortex LLMs with Prompt engineering\n- Visualising the Results using Pydeck and st.map\n\nAll the code / engineering was performed using our native snowpark dataframe capabilities inside a Snowflake notebook.  The exact same functions can also be performed using SQL instead\n\n## Next Step\n-  Next please revert to the **Generate Incidents** Streamlit app.  This will create ficticious incidents of what might happen if our concerns are ignored"
  }
 ]
}